---
title: "QVI_customer_analytics"
author: "Bena"
date: "`r Sys.Date()`"
output: html_document
---


# Setting up
Load required packages and libraries

```{r}
library(data.table)
library(ggplot2)
library(ggmosaic)
library(readr)
library(lubridate)
library(tidyverse)
```

# Importing data

```{r}
transactions <- read_csv("QVI_transaction_data.csv")
transactionData <- fread(paste0("QVI_transaction_data.csv"))
```

```{r}
customers <- read_csv("QVI_purchase_behaviour.csv")
customerData <- fread(paste0("QVI_purchase_behaviour.csv"))
```

# Understanding the data - Exploratory Data Analysis

Examining transaction data

```{r}
str(transactionData)
str(transactions)
skimr::skim_without_charts(transactions)
skimr::skim_without_charts(transactionData)
colnames(transactions)
colnames(transactionData)
```

The skim_without_charts() function revealed that there are no missing values or whitespaces. 
Thus, we have 264839 & 8 variables

```{r}
blanks <- transactionData %>%                       #confirmed there are no nulls
  filter(
    DATE == "NA",
    PROD_NAME == "NA"
    )
```

1. Amend data type for DATE column from chr 

```{r}
transactionData$DATE <- parse_date_time(transactionData$DATE, "mdy")  #converts to POSIXct
transactionData$DATE <- as.Date(transactionData$DATE, formats = "%y/%m/%d")  #converts the POSIXct to Date
```

2. Examine the PROD_NAME column

```{r}
product_summary <- transactionData %>%
  pull(PROD_NAME) %>%
  unique   #lists unique products

print(product_summary)
```

The data includes other products which aren't chips

3. Summarize the data to spot outliers & nulls

```{r}
summary(transactionData)
```


```{r}
productWords <- data.table(unlist(strsplit(unique(transactionData[, PROD_NAME]), " ")))
setnames(productWords, 'words')             #ignore, the product names are truncated & mixed up**

View(productWords)
```

Filter to remain with Chips products only

```{r}
pattern <- "[Cc]hip([^otle])|(?i)chp(\\s)|(?i)chps(\\w+)"
transactionData3 <- transactionData %>%
  mutate(CHIPS = str_detect(PROD_NAME,pattern)) %>%
  filter(CHIPS == "TRUE")

View(transactionData3)
```

The filtering where the words with any of the regexp pattern was found left us with 84188 observations which is 31.8% of the original transactions data
Export data to excel to confirm it's chips data only

```{r}
write_csv(transactionData3,"transactions_nochp3.csv")
```

4. Create size column from product name

```{r}
transactionData3 <- transactionData3 %>%
  mutate(SIZE = str_sub(PROD_NAME, -4, -1)) %>%
  mutate(SIZE = str_replace_all(SIZE, "g", ""))     # Remove "g" from size

transactionData3 <- transactionData3 %>%
  mutate(SIZE = as.numeric(SIZE))                   #converts `SIZE`from chr to numeric data type
```

Alternatively

```{r}
transactionData4 <- transactionData %>%
  mutate(size = parse_number(PROD_NAME))

View(transactionData4)
```

Remove the 'g' from size

```{r}
transactionData3 <- transactionData3 %>%
  mutate(PROD_NAME = str_replace_all(PROD_NAME, "([\\d+]g)", "")) %>%
  mutate(PROD_NAME = str_replace_all(PROD_NAME, "([\\d+])", ""))
```

```{r}
size_summary <- transactionData3 %>%
  group_by(SIZE) %>%
  summarise(min_size = min(SIZE),
            max_size = max(SIZE))

View(size_summary)
```

The package sizes look reasonable, thus ok to proceed

5. Check out products in `PROD_NAME

```{r}
unique_chips <- transactionData3 %>%
  pull(PROD_NAME) %>%
  unique
 
print(unique_chips)
```

Then get brand_names from `PROD_NAME`

```{r}
transactionData3 <- transactionData3 %>%
  mutate(brand = str_sub(PROD_NAME, 1, 4)) %>%
  mutate(brand_name = case_when(
      brand == "Natu" ~ "Natural Chips Co",
      brand == "Smit" ~ "Smiths Crinkles",
      brand == "Kett" ~ "Kettle Tortilla Chips",
      brand == "Thin" ~ "Thins Chips",
      brand == "Dori" ~ "Doritos Corn Chips",
      brand == "Cobs" ~ "Cobs Popd",
      brand == "Fren" ~ "French Fries",
      brand == "WW O" ~ "WW",
      brand == "WW D" ~ "WW",
      brand == "WW S" ~ "WW"))
```


6. Visualize the number bought for each brand & size

```{r}
transactionData3 %>%
  ggplot(aes(x = SIZE, fill = brand_name)) +
  geom_histogram(bins = 50)
```

We see that the most popular size & brand is Thins Chips, all packaged in 175g
Followed by Cobs Popd, all packaged in 110g & Doritos Corn Chips - 170g

a) More about Thins Chips

```{r}
thins <- transactionData3 %>%
  select(SIZE, brand_name) %>%
  filter(brand_name == "Thins Chips")

View(thins)
```

To confirm Thins Chips are only packaged in 175g

```{r}
thins <- thins %>%
  filter(SIZE != 175)
```

b) More about Cobs Popd

```{r}
cobs <- transactionData3 %>%
  select(SIZE, brand_name) %>%
  filter(brand_name == "Cobs Popd")
```

To confirm Cobs Popd are only packaged in 110g

```{r}
cobs <- cobs %>%
  filter(SIZE != 110)
```

c) More about Doritos Corn Chips

```{r}
doritos <- transactionData3 %>%
  select(SIZE, brand_name) %>%
  filter(brand_name == "Doritos Corn Chips")

View(doritos)
```

7. Counting frequency for each brand

```{r}
brand_summary <- transactionData3 %>%
  group_by(brand_name) %>%
  summarize(number_bought = n(),
            avg_qnty = mean(PROD_QTY),
            avg_sales = mean(TOT_SALES),
            min_qnty = min(PROD_QTY),
            max_qnty = max(PROD_QTY)) %>%
  arrange(-avg_sales)

View(brand_summary)
```

Minimum quantity for each brand is 1 and maximum quantity ranges between 3-5 except 
Doritos Corn Chips has a maximum quantity of 200 in one transaction. Which needs to be investigated further

```{r}
Q200 <- transactionData3 %>%
  filter(PROD_QTY == 200)

View(Q200)
```

2 obs have `PROD_QTY`=200, both were bought using the same `LYLTY_CARD_NBR`
Investigate whether the same customer has other purchases

```{r}
card_226000 <- transactionData3 %>%
  filter(LYLTY_CARD_NBR == 226000)

View(card_226000)
```

There are only 2 transactions for this customer and it can be assumed that it's not a retail customer. 
The transactions are also months apart, it can be concluded that it's not a regular purchase. 
We can therefore exclude these 2 obs from analysis because they are outliers.

```{r}
transactionData3 <- transactionData3 %>%
  filter(PROD_QTY != 200)
```

Re-do the brand summary

```{r}
brand_summary2 <- transactionData3 %>%
  group_by(brand_name) %>%
  summarize(transactions_no = n(),
            quantity_bought =sum(PROD_QTY),
            avg_qnty = mean(PROD_QTY),
            avg_sales = mean(TOT_SALES),
            min_qnty = min(PROD_QTY),
            max_qnty = max(PROD_QTY)) %>%
  arrange(-avg_sales)

View(brand_summary2)
```

Impact that the outlier had it:
 - reduced number_bought from 19059 to 19057  ** use total quantity??
 - reduced avg_qnty from 1.935988 to 1.915202
 - reduced avg_sales from 8.812073 to 8.744781
 
From the brand_summary, it is evident that most clients buy 2 packets of chips on average

# Trend Analysis

```{r}
date_summary <- transactionData3 %>%
  group_by(DATE) %>%
  summarise(tranx_per_day = n()) %>%
  arrange(DATE)
```

1. Sort the dates in ascending order

```{r}
date_summary <- date_summary %>%
  mutate(DATE = as.Date(DATE, "%m/%d/%Y")) %>%
  arrange(DATE)

View(date_summary)
```

Alternatively, create a sequence for dates then merge with date_summary

```{r}
alldates <- data.table(seq(as.Date("2018-07-01"),as.Date("2019-06-30"),by ="day"))
names(alldates)[1] <- "DATE"                             #to rename column from V1
```

```{r}
transactions_by_day <- merge(alldates, date_summary, all.x=TRUE)
```

2. Plot the dates

```{r}
ggplot(data = transactions_by_day, aes(x = DATE, y=tranx_per_day)) +
  geom_line() +
  labs(x = "day", y = "no. of transactions", title = "Transactions over time") +
  scale_x_date(breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

We can see there's a sharp increase in Dec & sharp decrease in Oct & Aug
Focus on these months & look at individual days

a) More about Dec data

```{r}
dec_summary <- transactions_by_day %>%
  filter(DATE >= '2018-12-01' & DATE <= '2018-12-31')

View(dec_summary)
```

Visualize Dec data only

```{r}
transactions_by_day %>%
  filter(DATE >= '2018-12-01' & DATE <= '2018-12-31') %>%
  ggplot(aes(x = DATE, y = tranx_per_day)) +
  geom_line() +
  labs(x = "date", y = "no. of transactions", title = "Dec transactions per day") +
  scale_x_date(breaks = "2 day") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

```

The sharp increase can be attributed to an increase in sales in the week leading to Christmas
Christmas also has 0 sales because it's a public holiday and in most cases, most if not all shops are closed on that day
This also explains why our date_summary has 364 entries as opposed to the 365 days in a year

b) More about Oct data

```{r}
oct_summary <- date_summary %>%
  filter(DATE >= '2018-10-01' & DATE <= '2018-10-31')

View(oct_summary)
```

Visualize Oct data only

```{r}
date_summary %>%
  filter(DATE >= '2018-10-01' & DATE <= '2018-10-31') %>%
  ggplot(aes(x = DATE, y = tranx_per_day)) +
  geom_line() +
  labs(x = "date", y = "no. of transactions", title = "October transactions per day")
```
or
```{r}
oct_summary %>%
  ggplot(aes(x = DATE, y = tranx_per_day)) +
  geom_line() +
  labs(x = "date", y = "no. of transactions", title = "October transactions per day")  
```

There was a dip on 18th which pulled down the sales in October. Why? **


We're happy with the transactions data. We'll proceed with data cleaning, manipulation & analysis of the customer data

Examining customer data

```{r}
View(customerData)
skimr::skim_without_charts(customerData)
```

There are no missing values or whitespaces. Thus, we'll be working with 72637 observations & 8 variables.

# Exploratory Data Analysis of customer data

```{r}
life_stage <- customerData %>%
  pull(LIFESTAGE) %>%
  unique

print(life_stage)
```

Above code displays unique entries in the LIFESTAGE column

A summary of `LIFESTAGE`

```{r}
life_stage <- customerData %>%
  group_by(LIFESTAGE) %>%
  summarise(number = n()) %>%
  arrange(-number)

View(life_stage)

customerData[, .N, by = LIFESTAGE][order(-N)]
```

The highest number of customers fall under the RETIREES category, followed by OLDER SINGLES/COUPLES. 
The least number of customers are in NEW FAMILIES category

Display unique entries in `PREMIUM_CUSTOMER` column 

```{r}
customer_category <- customerData %>%
  pull(PREMIUM_CUSTOMER) %>%
  unique

print(customer_category)
```

A summary of `PREMIUM_CUSTOMER` column

```{r}
customer_category <- customerData %>%
  group_by(PREMIUM_CUSTOMER) %>%
  summarize(number = n()) %>%
  arrange(-number)

View(customer_category)

customerData[, .N, by = PREMIUM_CUSTOMER][order(-N)]
```

We see the most customers are in Mainstream segment, followed by Budget then Premium customers

Customer summary according to `LYLTY_CARD_NBR`

```{r}
customer_summary <- customerData %>%
group_by(LYLTY_CARD_NBR) %>%
summarise(tranxn_no = n())

View(customer_summary)
```

```{r}
customer_summary1 <- customer_summary %>%
  filter(tranxn_no != 1)
```

There's only transaction per card

# Merge customerData & transactionData

Since we want to keep all observations in transactionData we'll use the left join to merge with the customerData

```{r}
combined_data <- merge(transactionData3, customerData, all.x = TRUE)

View(combined_data)
```
or
```{r}
combined_data2 <- transactionData3 %>%
  left_join(customerData, by = "LYLTY_CARD_NBR")

View(combined_data2)
```

1. Delete unnecessary columns

```{r}
combined_data <- combined_data %>%
  select(c(-9,-11))                               #deleted CHIPS & brand columns respectively
```

2. Standardize column names

```{r}
names(combined_data) <- tolower(names(combined_data))
```

3. Rename columns so they're easier to remember

```{r}
names(combined_data)[1] <- "loyalty_card_number"    #from lylty_card_nbr
names(combined_data)[5] <- "product_number"      #from prod_nbr
names(combined_data)[6] <- "product_name"      #from prod_name
names(combined_data)[7] <- "product_quantity"      #from prod_qty
names(combined_data)[8] <- "total_sales"      #from tot_sales
names(combined_data)[9] <- "package_size"      #from size
names(combined_data)[12] <- "segment"      #from premium_customer
```

4. Confirm if there's any customer who wasn't matched to a transaction

```{r}
combined_data3 <- combined_data %>%
  filter(segment == "NA" | lifestage == "NA")               
```

Alternative method

```{r}
combined_data3[is.null(lifestage), .N]
combined_data3[is.null(segment), .N]
```

5. Save merged data frame for later using write_csv or fwrite()

```{r}
fwrite(combined_data3, paste0("QVI_data.csv"))
```

Data preparation is over now, time for data analysis on customer segments
# Define some metrics

a) Calculate total sales by `lifestage` and `segment` & 
plot the split by these segments to describe which customer segment contributes most to chip sales.

```{r}
total_sales_lifestage <- combined_data %>%
  group_by(lifestage, segment) %>%
  summarise(total_sales = sum(total_sales)) %>%
  arrange(-total_sales)
```

or

```{r}
total_sales_lifestage2 <- combined_data[, .(sales = sum(total_sales)), .(lifestage, segment)]
```

A visualization for the same

```{r}
combined_data %>%
  ggplot(aes(x = lifestage, y = total_sales, fill = segment)) +
  geom_col() +
  labs(y = "total_sales", title = "Total sales per customer segment") +     #exact figures to show on y-axis**
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

The customer category with the most sales are older families - budget segment & young singles/couples - mainstream segment and Mainstream Retirees. 
Alternatively, plot using

```{r}
p <- ggplot(data = total_sales_lifestage2) +                
  geom_mosaic(aes(weight = sales, x = product(segment, lifestage), fill = segment)) +
  labs(x = "Lifestage", y = "Premium customer flag", title = "Proportion of sales") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
 
p + geom_text(data = ggplot_build(p)$data[[1]], 
            aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = as.character(paste(round(.wt/sum(.wt),3)*100,'%'))))
```

Investigate if the higher sales are due to there being more customers who buy chips

b) Number of customers by lifestage and segment

```{r}
segment_tranxns <- combined_data %>%
  group_by(lifestage, segment)%>%
  summarise(number = n()) %>%
  arrange(-number)

View(segment_tranxns)
```

Try with wider data

```{r}
segment_tranxns_wider <- segment_tranxns %>%
  pivot_wider(names_from = segment,
              values_from = number)        #longer data has better visibility, sort in descending order

View(segment_tranxns_wider)
```

Alternatively use

```{r}
customers <- combined_data[, .(customers = uniqueN(loyalty_card_number)), .(lifestage, segment)][order(-customers)]
View(customers)
```

Visualize using

```{r}
q <- ggplot(data = customers) +                         
  geom_mosaic(aes(weight = customers, x = product(segment,lifestage), fill = segment)) +
  labs(x = "Lifestage", y = "segment", title = "Proportion of customers") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

q + geom_text(data = ggplot_build(q)$data[[1]], 
            aes(x = (xmin + xmax)/2 , y = (ymin + ymax)/2, label = as.character(paste(round(.wt/sum(.wt),3)*100,'%'))))
```


There are more mainstream - YOUNG SINGLES/COUPLES & mainstream RETIREES who buy chips. This contributes to more sales in these 2 categories but doesn't seem to be the main driver of sales in Budget - OLDER FAMILIES.
 
This implies it's not about having more customers who buy chips. If not, then let's consider quantity bought

c) Consider,  average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER

```{r}
segment_qnty <- combined_data %>%
  group_by(lifestage, segment)%>%
  summarise(avg_qnty = mean(product_quantity)) %>%
  arrange(-avg_qnty)

View(segment_qnty)
```

Calculate average manually, considering unique cards only

```{r}
segment_qnty2 <- combined_data[, .(avg_qnty = sum(product_quantity)/uniqueN(loyalty_card_number)),.(lifestage, segment)][order(-avg_qnty)] 
```

Visualize

```{r}
ggplot(data = segment_qnty2, aes(weight = avg_qnty, x = lifestage, fill = segment)) +
  geom_bar(position = position_dodge()) +
  labs(x = "Lifestage", y = "Avg units per transaction", title = "Quantity per customer") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

Generally, Older & Young families buy more quantities on average

d) Let's also find out the average price in each customer segment since it's a driver of total sales

```{r}
segment_price_avg <- combined_data %>%
  group_by(lifestage, segment)%>%
  summarise(avg_price = mean(total_sales)) %>%
  arrange(-avg_price)

View(segment_price_avg)
```

Calculate average price manually

```{r}
segment_price_avg2 <- combined_data[, .(avg_price = sum(total_sales)/sum(product_quantity)), .(lifestage, segment)][order(-avg_price)]
```

Visualize using segment_price_avg2

```{r}
ggplot(data = segment_price_avg2, 
       aes(weight = avg_price, x = lifestage, fill = segment)) +
  geom_bar(position = position_dodge()) +
  labs(x = "Lifestage", y = "Avg price per unit", title = "Price per unit") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

On average, Mainstream - YOUNG SINGLES/COUPLES and MIDAGE SINGLES/COUPLES are willing to spend more on a packet of chips, compared to their premium & budget counterparts. 

Mainstream could in other words be referred to as middle_income class. Premium and budget refer to high-income and low-income socioeconomic classes respectively

In that case then, this could be explained by the fact that premium customers are more likely to purchase healthier snacks & occasionally they buy chips for "entertainment" purposes.
This is also supported by there being fewer Premium - YOUNG SINGLES/COUPLES and MIDAGE SINGLES/COUPLES buying chips compared to their Mainstream counterparts.

We can confirm if price per unit is statistically significant since the difference in avg_price isn't large.

# Statistical analysis

We can  perform an independent t-test between mainstream vs premium & budget MIDAGE SINGLES/COUPLES & YOUNG SINGLES/COUPLES

```{r}
pricePerUnit <- combined_data[, price := total_sales/product_quantity]

t.test(combined_data[lifestage %in% c("YOUNG SINGLES/COUPLES", "MIDAGE SINGLES/COUPLES") & segment == "Mainstream", price]
       , combined_data[lifestage %in% c("YOUNG SINGLES/COUPLES", "MIDAGE SINGLES/COUPLES") & segment != "Mainstream", price]
       , alternative = "greater")
```

The t-test results in a p-value < 2.2e-16 which is statistically significant.
The t-test is used to test the hypothesis that unit price for Mainstream, YOUNG SINGLES/COUPLES and MIDAGE SINGLES/COUPLES is significantly higher than that of Budget or Premium, YOUNG SINGLES/COUPLES and MIDAGE SINGLES/COUPLES.

We might want to target customer segments that contribute the most sales e.g Mainstream, YOUNG SINGLES/COUPLES.
Let's focus on that category since they featured highly in 3/4 metrics i.e. their proportion of sales, customers and price per unit. 

```{r}
mainstream_YSC <- combined_data[lifestage == "YOUNG SINGLES/COUPLES" & segment == "Mainstream",]
other_segments <- combined_data[!(lifestage == "YOUNG SINGLES/COUPLES" & segment == "Mainstream"),]
```

a) which brands do they tend to buy most?

```{r}
MYSG_brand <- combined_data %>%
  filter(segment == "Mainstream" & lifestage == "YOUNG SINGLES/COUPLES") %>%
  group_by(brand_name) %>%
  summarise(number = n()) %>%
  arrange(-number)

View(MYSG_brand)
```

We can see that Doritos Corn Chips tops the list followed by Thins Chips
We can also use the affinity analysis or a-priori analysis to find out their most preferred brand **

```{r}
quantity_MYSG <- mainstream_YSC[, sum(product_quantity)]
print(quantity_MYSG)
```

```{r}
quantity_others <- other_segments[, sum(product_quantity)]
print(quantity_others)
```

```{r}
quantity_MYSG_by_brand <- mainstream_YSC[, .(MYSG = sum(product_quantity)/quantity_MYSG), by = brand_name]
quantity_other_by_brand <- other_segments[, .(other = sum(product_quantity)/quantity_others), by = brand_name]
```

```{r}
brand_proportions <- merge(quantity_MYSG_by_brand, quantity_other_by_brand)[, affinityToBrand := MYSG/other]
brand_proportions[order(-affinityToBrand)]
```

We see that:
- Mainstream YOUNG SINGLES/COUPLES are 23% more likely to purchase Doritos Corn Chips compared to the other segments
- Mainstream YOUNG SINGLES/COUPLES are 50% less likely to purchase WW Chips compared to the other segments

b) Which package_size do they tend to buy most?

```{r}
MYSG_size <- combined_data %>%
  filter(segment == "Mainstream" & lifestage == "YOUNG SINGLES/COUPLES") %>%
  group_by(package_size) %>%
  summarise(number = n()) %>%
  arrange(-number)

View(MYSG_size)
```

We can see that 175g tops the list followed by 150g

# Conlcusion

In summary, we’ve noted the following:
i) Larger proportions of sales are from the Budget - OLDER FAMILIES, Mainstream - YOUNG SINGLES/COUPLES, and Mainstream - RETIREES customers. 
ii) The high sales proportion by Mainstream - YOUNG SINGLES/COUPLES is due to there being more of them compared to other buyers. 
iii) Mainstream - YOUNG SINGLES/COUPLES are also likely to pay more per packet of chips compared to other customer categories. This suggests that there could be more impulsive buying among clients in this category.
iv) Mainstream - YOUNG SINGLES/COUPLES are 23% more likely to buy Doritos Corn Chips compared to the rest of the population. 


# Recommendation & next steps:
i) The category manager may strategically place the Doritos Corn Chips near shelves that are most frequented by Mainstream - YOUNG SINGLES/COUPLES, especially the smaller sizes packaged in 150g & 170g.
ii) Quantium can help the Category Manager with recommendations of where these shelves are and further help them with measuring the impact of the changed placement.


